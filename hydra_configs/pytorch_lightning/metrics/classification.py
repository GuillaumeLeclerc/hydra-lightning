# Autogenerated by configen, do not edit.
# If encountering an error, please file an issue @
# https://github.com/romesco/hydra-lightning 
# fmt: off
# isort: skip_file
# flake8: noqa
# Hydra + Lightning:

from dataclasses import dataclass
from typing import Optional
import pytorch_lightning as pl
from packaging import version
import importlib

def override_imports_for_legacy():
    if version.parse(pl.__version__) < version.parse("1.0.0"):
        module = importlib.import_module('hydra_configs.pytorch_lightning_v090.metrics.classification')
        globals().update(
            {n: getattr(module, n) for n in module.__all__} if hasattr(module, '__all__') else 
            {k: v for (k, v) in module.__dict__.items() if not k.startswith('_')}
        )


@dataclass
class AccuracyConf:
    _target_: str = "pytorch_lightning.metrics.classification.Accuracy"
    threshold: float = 0.5
    compute_on_step: bool = True
    dist_sync_on_step: bool = False
    process_group: Any = None


@dataclass
class FbetaConf:
    _target_: str = "pytorch_lightning.metrics.classification.Fbeta"
    num_classes: int = 1
    beta: float = 1.0
    threshold: float = 0.5
    average: str = "micro"
    multilabel: bool = False
    compute_on_step: bool = True
    dist_sync_on_step: bool = False
    process_group: Any = None


@dataclass
class PrecisionConf:
    _target_: str = "pytorch_lightning.metrics.classification.Precision"
    num_classes: int = 1
    threshold: float = 0.5
    average: str = "micro"
    multilabel: bool = False
    compute_on_step: bool = True
    dist_sync_on_step: bool = False
    process_group: Any = None

override_imports_for_legacy()
